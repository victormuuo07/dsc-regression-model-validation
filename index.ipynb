{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Regression Model Validation\n","\n","## Introduction\n","\n","Previously you've evaluated a multiple linear regression model by calculating metrics based on the fit of the training data. In this lesson you'll learn why it's important to split your data in a train and a test set if you want to evaluate a model used for prediction.\n","\n","## Objectives\n","\n","You will be able to:\n","\n","* Perform a train-test split\n","* Prepare training and testing data for modeling\n","* Compare training and testing errors to determine if model is over or underfitting"]},{"cell_type":"markdown","metadata":{},"source":["## Model Evaluation\n","\n","Recall some ways that we can evaluate linear regression models.\n","\n","### Residuals\n","\n","It is pretty straightforward that, to evaluate the model, you'll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the **residuals**:\n","\n","$r_{i} = y_{i} - \\hat y_{i}$ \n","\n","To get a summarized measure over all the instances, a popular metric is the (Root) Mean Squared Error:\n","\n","RMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\n","\n","MSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n","\n","Larger (R)MSE indicates a _worse_ model fit."]},{"cell_type":"markdown","metadata":{},"source":["## The Need for Train-Test Split\n","\n","### Making Predictions and Evaluation\n","\n","So far we've simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\n","\n","Let's say we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to **predict** miles per gallon for a new car that isn't part of our dataset, or predict the price for a new house in Ames.\n","\n","In order to get a good sense of how well your model will be doing on new instances, you'll have to perform a so-called \"train-test-split\". What you'll be doing here, is taking a sample of the data that serves as input to \"train\" our model - fit a linear regression and compute the parameter estimates for our variables, and then calculate how well our predictive performance is doing based solely on the \"test\" data, comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\n","\n","### Underfitting and Overfitting\n","\n","Another reason to use train-test-split is because of a common problem which doesn't only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:"]},{"cell_type":"markdown","metadata":{},"source":["<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_overfit_underfit.png'>"]},{"cell_type":"markdown","metadata":{},"source":["## Mechanics of Train-Test Split\n","\n","When performing a train-test-split, it is important that the data is **randomly** split. At some point, you will encounter datasets that have certain characteristics that are only present in certain segments of the data. For example, if you were looking at sales data for a website, you might expect the data to look different on days that promotional deals were held versus days that deals were not held. If we don't randomly split the data, there is a chance we might overfit to the characteristics of certain segments of data.\n","\n","Another thing to consider is just **how big** each training and testing set should be. There is no hard and fast rule for deciding the correct size, but the range of training set is usually anywhere from 66% - 80% (and testing set between 33% and 20%). Some types of machine learning models need a substantial amount of data to train on, and as such, the training sets should be larger. Some models with many different tuning parameters will need to be validated with larger sets (the test size should be larger) to determine what the optimal parameters should be. When in doubt, just stick with training set sizes around 70% and test set sizes around 30%."]},{"cell_type":"markdown","metadata":{},"source":["## Train-Test Split with Scikit-Learn\n","\n","You could write your own pandas code to shuffle and split your data, but we'll use the convenient `train_test_split` function from scikit-learn instead. We'll also use the Auto MPG dataset."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["<bound method DataFrame.info of       mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n","0    18.0          8         307.0         130    3504          12.0   \n","1    15.0          8         350.0         165    3693          11.5   \n","2    18.0          8         318.0         150    3436          11.0   \n","3    16.0          8         304.0         150    3433          12.0   \n","4    17.0          8         302.0         140    3449          10.5   \n","..    ...        ...           ...         ...     ...           ...   \n","387  27.0          4         140.0          86    2790          15.6   \n","388  44.0          4          97.0          52    2130          24.6   \n","389  32.0          4         135.0          84    2295          11.6   \n","390  28.0          4         120.0          79    2625          18.6   \n","391  31.0          4         119.0          82    2720          19.4   \n","\n","     model year  origin                   car name  \n","0            70       1  chevrolet chevelle malibu  \n","1            70       1          buick skylark 320  \n","2            70       1         plymouth satellite  \n","3            70       1              amc rebel sst  \n","4            70       1                ford torino  \n","..          ...     ...                        ...  \n","387          82       1            ford mustang gl  \n","388          82       2                  vw pickup  \n","389          82       1              dodge rampage  \n","390          82       1                ford ranger  \n","391          82       1                 chevy s-10  \n","\n","[392 rows x 9 columns]>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","data = pd.read_csv('auto-mpg.csv')\n","data.head()\n","\n","data.info"]},{"cell_type":"markdown","metadata":{},"source":["The `train_test_split` function ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) takes in a series of array-like variables, as well as some optional arguments. It returns multiple arrays.\n","\n","For example, this would be a valid way to use `train_test_split`:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train, test = train_test_split(data)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["<bound method DataFrame.info of       mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n","157  14.0          8         351.0         148    4657          13.5   \n","182  25.0          4         140.0          92    2572          14.9   \n","290  18.5          8         360.0         150    3940          13.0   \n","174  19.0          6         232.0          90    3211          17.0   \n","373  36.0          4          98.0          70    2125          17.3   \n","..    ...        ...           ...         ...     ...           ...   \n","106  18.0          6         232.0         100    2789          15.0   \n","74   14.0          8         318.0         150    4077          14.0   \n","310  37.2          4          86.0          65    2019          16.4   \n","78   26.0          4          96.0          69    2189          18.0   \n","322  40.8          4          85.0          65    2110          19.2   \n","\n","     model year  origin                              car name  \n","157          75       1                              ford ltd  \n","182          76       1                              capri ii  \n","290          79       1  chrysler lebaron town @ country (sw)  \n","174          75       1                             amc pacer  \n","373          82       1                        mercury lynx l  \n","..          ...     ...                                   ...  \n","106          73       1                           amc gremlin  \n","74           72       1        plymouth satellite custom (sw)  \n","310          80       3                            datsun 310  \n","78           72       2                       renault 12 (sw)  \n","322          80       3                            datsun 210  \n","\n","[294 rows x 9 columns]>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train.head()\n","train.info"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["<bound method DataFrame.info of       mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n","271  23.9          4         119.0          97    2405          14.9   \n","379  38.0          4          91.0          67    1995          16.2   \n","229  15.5          8         400.0         190    4325          12.2   \n","311  28.0          4         151.0          90    2678          16.5   \n","146  26.0          4         116.0          75    2246          14.0   \n","..    ...        ...           ...         ...     ...           ...   \n","276  31.5          4          89.0          71    1990          14.9   \n","34   17.0          6         250.0         100    3329          15.5   \n","376  34.0          4         108.0          70    2245          16.9   \n","196  33.0          4          91.0          53    1795          17.4   \n","323  44.3          4          90.0          48    2085          21.7   \n","\n","     model year  origin                   car name  \n","271          78       3              datsun 200-sx  \n","379          82       3              datsun 310 gx  \n","229          77       1           chrysler cordoba  \n","311          80       1         chevrolet citation  \n","146          74       2                fiat 124 tc  \n","..          ...     ...                        ...  \n","276          78       2        volkswagen scirocco  \n","34           71       1  chevrolet chevelle malibu  \n","376          82       3             toyota corolla  \n","196          76       3                honda civic  \n","323          80       2       vw rabbit c (diesel)  \n","\n","[98 rows x 9 columns]>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["test.head()\n","test.info"]},{"cell_type":"markdown","metadata":{},"source":["In this case, the DataFrame `data` was split into two DataFrames called `train` and `test`. `train` has 294 values (75% of the full dataset) and `test` has 98 values (25% of the full dataset). Note the randomized order of the index values on the left.\n","\n","However you can also pass multiple array-like variables into `train_test_split` at once. For each variable that you pass in, you will get a train and a test copy back out.\n","\n","Most commonly in this curriculum these are the inputs and outputs:\n","\n","Inputs\n","\n","- `X`\n","- `y`\n","\n","Outputs\n","\n","- `X_train`\n","- `X_test`\n","- `y_train`\n","- `y_test`"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["y = data[['mpg']]\n","X = data.drop(['mpg', 'car name'], axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cylinders</th>\n","      <th>displacement</th>\n","      <th>horsepower</th>\n","      <th>weight</th>\n","      <th>acceleration</th>\n","      <th>model year</th>\n","      <th>origin</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>37</th>\n","      <td>8</td>\n","      <td>350.0</td>\n","      <td>165</td>\n","      <td>4209</td>\n","      <td>12.0</td>\n","      <td>71</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>310</th>\n","      <td>4</td>\n","      <td>86.0</td>\n","      <td>65</td>\n","      <td>2019</td>\n","      <td>16.4</td>\n","      <td>80</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>440.0</td>\n","      <td>215</td>\n","      <td>4312</td>\n","      <td>8.5</td>\n","      <td>70</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>281</th>\n","      <td>6</td>\n","      <td>232.0</td>\n","      <td>90</td>\n","      <td>3265</td>\n","      <td>18.2</td>\n","      <td>79</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>375</th>\n","      <td>4</td>\n","      <td>107.0</td>\n","      <td>75</td>\n","      <td>2205</td>\n","      <td>14.5</td>\n","      <td>82</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     cylinders  displacement  horsepower  weight  acceleration  model year  \\\n","37           8         350.0         165    4209          12.0          71   \n","310          4          86.0          65    2019          16.4          80   \n","7            8         440.0         215    4312           8.5          70   \n","281          6         232.0          90    3265          18.2          79   \n","375          4         107.0          75    2205          14.5          82   \n","\n","     origin  \n","37        1  \n","310       3  \n","7         1  \n","281       1  \n","375       3  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X_train.head()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mpg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>37</th>\n","      <td>14.0</td>\n","    </tr>\n","    <tr>\n","      <th>310</th>\n","      <td>37.2</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>14.0</td>\n","    </tr>\n","    <tr>\n","      <th>281</th>\n","      <td>20.2</td>\n","    </tr>\n","    <tr>\n","      <th>375</th>\n","      <td>36.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      mpg\n","37   14.0\n","310  37.2\n","7    14.0\n","281  20.2\n","375  36.0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["y_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["We can view the lengths of the results like this:"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["294 98 294 98\n"]}],"source":["print(len(X_train), len(X_test), len(y_train), len(y_test))"]},{"cell_type":"markdown","metadata":{},"source":["However it is not recommended to pass in just the data to be split. This is because the randomization of the split means that you will get different results for `X_train` etc. every time you run the code. **For reproducibility, it is always recommended that you specify a `random_state`**, such as in this example:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["Another optional argument is `test_size`, which makes it possible to choose the size of the test set and the training set instead of using the default 75% train/25% test proportions."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["Note that the lengths of the resulting datasets will be different:"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["313 79 313 79\n"]}],"source":["print(len(X_train), len(X_test), len(y_train), len(y_test))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cylinders</th>\n","      <th>displacement</th>\n","      <th>horsepower</th>\n","      <th>weight</th>\n","      <th>acceleration</th>\n","      <th>model year</th>\n","      <th>origin</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>258</th>\n","      <td>6</td>\n","      <td>225.0</td>\n","      <td>110</td>\n","      <td>3620</td>\n","      <td>18.7</td>\n","      <td>78</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>182</th>\n","      <td>4</td>\n","      <td>140.0</td>\n","      <td>92</td>\n","      <td>2572</td>\n","      <td>14.9</td>\n","      <td>76</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>172</th>\n","      <td>6</td>\n","      <td>171.0</td>\n","      <td>97</td>\n","      <td>2984</td>\n","      <td>14.5</td>\n","      <td>75</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>8</td>\n","      <td>318.0</td>\n","      <td>150</td>\n","      <td>4135</td>\n","      <td>13.5</td>\n","      <td>72</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>4</td>\n","      <td>86.0</td>\n","      <td>64</td>\n","      <td>1875</td>\n","      <td>16.4</td>\n","      <td>81</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     cylinders  displacement  horsepower  weight  acceleration  model year  \\\n","258          6         225.0         110    3620          18.7          78   \n","182          4         140.0          92    2572          14.9          76   \n","172          6         171.0          97    2984          14.5          75   \n","63           8         318.0         150    4135          13.5          72   \n","340          4          86.0          64    1875          16.4          81   \n","\n","     origin  \n","258       1  \n","182       1  \n","172       1  \n","63        1  \n","340       1  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["X_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Data for Modeling\n","\n","When using a train-test split, data preparation should happen _after_ the split. This is to avoid ***data leakage***. The general idea is that the treatment of the test data should be as similar as possible to how genuinely unknown data should be treated. And genuinely unknown data would not have been there at the time of fitting the scikit-learn transformers, just like it would not have been there at the time of fitting the model!\n","\n","In some cases you will see all of the data being prepared together for expediency, but the best practice is to prepare it separately.\n","\n","### Log Transformation"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>log_disp</th>\n","      <th>log_hp</th>\n","      <th>log_wt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>258</th>\n","      <td>5.416100</td>\n","      <td>4.700480</td>\n","      <td>8.194229</td>\n","    </tr>\n","    <tr>\n","      <th>182</th>\n","      <td>4.941642</td>\n","      <td>4.521789</td>\n","      <td>7.852439</td>\n","    </tr>\n","    <tr>\n","      <th>172</th>\n","      <td>5.141664</td>\n","      <td>4.574711</td>\n","      <td>8.001020</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>5.762051</td>\n","      <td>5.010635</td>\n","      <td>8.327243</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>4.454347</td>\n","      <td>4.158883</td>\n","      <td>7.536364</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     log_disp    log_hp    log_wt\n","258  5.416100  4.700480  8.194229\n","182  4.941642  4.521789  7.852439\n","172  5.141664  4.574711  8.001020\n","63   5.762051  5.010635  8.327243\n","340  4.454347  4.158883  7.536364"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import FunctionTransformer\n","import numpy as np\n","\n","# Instantiate a custom transformer for log transformation \n","log_transformer = FunctionTransformer(np.log, validate=True)\n","\n","# Columns to be log transformed \n","log_columns = ['displacement', 'horsepower', 'weight']\n","\n","# New names for columns after transformation\n","new_log_columns = ['log_disp', 'log_hp', 'log_wt']\n","\n","# Log transform the training columns and convert them into a DataFrame \n","X_train_log = pd.DataFrame(log_transformer.fit_transform(X_train[log_columns]), \n","                           columns=new_log_columns, index=X_train.index)\n","\n","X_train_log.head()"]},{"cell_type":"code","execution_count":17,"metadata":{"scrolled":false},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>log_disp</th>\n","      <th>log_hp</th>\n","      <th>log_wt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>78</th>\n","      <td>4.564348</td>\n","      <td>4.234107</td>\n","      <td>7.691200</td>\n","    </tr>\n","    <tr>\n","      <th>274</th>\n","      <td>4.795791</td>\n","      <td>4.744932</td>\n","      <td>7.935587</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>4.510860</td>\n","      <td>4.094345</td>\n","      <td>7.495542</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>4.510860</td>\n","      <td>4.248495</td>\n","      <td>7.578145</td>\n","    </tr>\n","    <tr>\n","      <th>387</th>\n","      <td>4.941642</td>\n","      <td>4.454347</td>\n","      <td>7.933797</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     log_disp    log_hp    log_wt\n","78   4.564348  4.234107  7.691200\n","274  4.795791  4.744932  7.935587\n","246  4.510860  4.094345  7.495542\n","55   4.510860  4.248495  7.578145\n","387  4.941642  4.454347  7.933797"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Log transform the test columns and convert them into a DataFrame \n","X_test_log = pd.DataFrame(log_transformer.transform(X_test[log_columns]), \n","                          columns=new_log_columns, index=X_test.index)\n","\n","X_test_log.head()"]},{"cell_type":"markdown","metadata":{},"source":["### One-Hot Encoding"]},{"cell_type":"code","execution_count":18,"metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>258</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>182</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>172</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       0    1\n","258  0.0  0.0\n","182  0.0  0.0\n","172  0.0  0.0\n","63   0.0  0.0\n","340  0.0  0.0"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","# Instantiate OneHotEncoder\n","# Need to use sparse_output=False for sklearn 1.2 or greater\n","ohe = OneHotEncoder(drop='first', sparse=False)\n","\n","# Create X_cat which contains only the categorical variables\n","cat_columns = ['origin']\n","X_train_cat = X_train.loc[:, cat_columns]\n","\n","# Transform training set\n","X_train_ohe = pd.DataFrame(ohe.fit_transform(X_train_cat),\n","                           index=X_train.index)\n","X_train_ohe.head()"]},{"cell_type":"code","execution_count":19,"metadata":{"scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cylinders</th>\n","      <th>acceleration</th>\n","      <th>model year</th>\n","      <th>log_disp</th>\n","      <th>log_hp</th>\n","      <th>log_wt</th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>258</th>\n","      <td>6</td>\n","      <td>18.7</td>\n","      <td>78</td>\n","      <td>5.416100</td>\n","      <td>4.700480</td>\n","      <td>8.194229</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>182</th>\n","      <td>4</td>\n","      <td>14.9</td>\n","      <td>76</td>\n","      <td>4.941642</td>\n","      <td>4.521789</td>\n","      <td>7.852439</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>172</th>\n","      <td>6</td>\n","      <td>14.5</td>\n","      <td>75</td>\n","      <td>5.141664</td>\n","      <td>4.574711</td>\n","      <td>8.001020</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>8</td>\n","      <td>13.5</td>\n","      <td>72</td>\n","      <td>5.762051</td>\n","      <td>5.010635</td>\n","      <td>8.327243</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>4</td>\n","      <td>16.4</td>\n","      <td>81</td>\n","      <td>4.454347</td>\n","      <td>4.158883</td>\n","      <td>7.536364</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     cylinders  acceleration  model year  log_disp    log_hp    log_wt    0  \\\n","258          6          18.7          78  5.416100  4.700480  8.194229  0.0   \n","182          4          14.9          76  4.941642  4.521789  7.852439  0.0   \n","172          6          14.5          75  5.141664  4.574711  8.001020  0.0   \n","63           8          13.5          72  5.762051  5.010635  8.327243  0.0   \n","340          4          16.4          81  4.454347  4.158883  7.536364  0.0   \n","\n","       1  \n","258  0.0  \n","182  0.0  \n","172  0.0  \n","63   0.0  \n","340  0.0  "]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Drop transformed columns\n","cols_to_drop = log_columns + cat_columns\n","X_train = X_train.drop(columns=cols_to_drop)\n","\n","# Combine the three datasets into training\n","X_train_tr = pd.concat([X_train, X_train_log, X_train_ohe], axis=1)\n","X_train_tr.head()"]},{"cell_type":"code","execution_count":20,"metadata":{"scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>78</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>274</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>387</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       0    1\n","78   1.0  0.0\n","274  1.0  0.0\n","246  0.0  1.0\n","55   0.0  0.0\n","387  0.0  0.0"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Transform testing set\n","X_test_ohe = pd.DataFrame(ohe.transform(X_test[cat_columns]),\n","                          index=X_test.index)\n","X_test_ohe."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cylinders</th>\n","      <th>acceleration</th>\n","      <th>model year</th>\n","      <th>log_disp</th>\n","      <th>log_hp</th>\n","      <th>log_wt</th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>78</th>\n","      <td>4</td>\n","      <td>18.0</td>\n","      <td>72</td>\n","      <td>4.564348</td>\n","      <td>4.234107</td>\n","      <td>7.691200</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>274</th>\n","      <td>4</td>\n","      <td>15.7</td>\n","      <td>78</td>\n","      <td>4.795791</td>\n","      <td>4.744932</td>\n","      <td>7.935587</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>4</td>\n","      <td>16.4</td>\n","      <td>78</td>\n","      <td>4.510860</td>\n","      <td>4.094345</td>\n","      <td>7.495542</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>4</td>\n","      <td>20.5</td>\n","      <td>71</td>\n","      <td>4.510860</td>\n","      <td>4.248495</td>\n","      <td>7.578145</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>387</th>\n","      <td>4</td>\n","      <td>15.6</td>\n","      <td>82</td>\n","      <td>4.941642</td>\n","      <td>4.454347</td>\n","      <td>7.933797</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     cylinders  acceleration  model year  log_disp    log_hp    log_wt    0  \\\n","78           4          18.0          72  4.564348  4.234107  7.691200  1.0   \n","274          4          15.7          78  4.795791  4.744932  7.935587  1.0   \n","246          4          16.4          78  4.510860  4.094345  7.495542  0.0   \n","55           4          20.5          71  4.510860  4.248495  7.578145  0.0   \n","387          4          15.6          82  4.941642  4.454347  7.933797  0.0   \n","\n","       1  \n","78   0.0  \n","274  0.0  \n","246  1.0  \n","55   0.0  \n","387  0.0  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["X_test = X_test.drop(columns=cols_to_drop)\n","\n","# Combine test set\n","X_test_tr = pd.concat([X_test, X_test_log, X_test_ohe], axis=1)\n","X_test_tr.head()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["X_train_tr.columns = X_train_tr.columns.astype(str)\n","X_test_tr.columns = X_test_tr.columns.astype(str)"]},{"cell_type":"markdown","metadata":{},"source":["## Building, Evaluating, and Validating a Model\n","\n","Great, now that you have preprocessed all the columns, you can fit a linear regression model: "]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","linreg = LinearRegression()\n","linreg.fit(X_train_tr, y_train)\n","\n","y_hat_train = linreg.predict(X_train_tr)\n","y_hat_test = linreg.predict(X_test_tr)"]},{"cell_type":"markdown","metadata":{},"source":["Look at the residuals and calculate the MSE for training and test sets:  "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["train_residuals = y_hat_train - y_train\n","test_residuals = y_hat_test - y_test"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Mean Squared Error: mpg    9.091819\n","dtype: float64\n","Test Mean Squared Error: mpg    10.010059\n","dtype: float64\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\user\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n","  return reduction(axis=axis, out=out, **passkwargs)\n"]}],"source":["mse_train = np.sum((y_train - y_hat_train)**2)/len(y_train)\n","mse_test = np.sum((y_test - y_hat_test)**2)/len(y_test)\n","print('Train Mean Squared Error:', mse_train)\n","print('Test Mean Squared Error:', mse_test)"]},{"cell_type":"markdown","metadata":{},"source":["You can also do this directly using sklearn's `mean_squared_error()` function: "]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Mean Squared Error: 9.091818811315937\n","Test Mean Squared Error: 10.010059484009501\n"]}],"source":["from sklearn.metrics import mean_squared_error\n","\n","train_mse = mean_squared_error(y_train, y_hat_train)\n","test_mse = mean_squared_error(y_test, y_hat_test)\n","print('Train Mean Squared Error:', train_mse)\n","print('Test Mean Squared Error:', test_mse)"]},{"cell_type":"markdown","metadata":{},"source":["Great, there does not seem to be a big difference between the train and test MSE!\n","\n","In other words, our evaluation process has indicated that we are **not** overfitting. In fact, we may be _underfitting_ because linear regression is not a very complex model."]},{"cell_type":"markdown","metadata":{},"source":["## Overfitting with a Different Model\n","\n","Just for the sake of example, here is a model that is overfit to the data. Don't worry about the model algorithm being shown! Instead, just look at the MSE for the train vs. test set, using the same preprocessed data:"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Mean Squared Error: 0.0\n","Test Mean Squared Error: 11.403164556962025\n"]}],"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","other_model = DecisionTreeRegressor(random_state=42)\n","other_model.fit(X_train_tr, y_train)\n","\n","other_train_mse = mean_squared_error(y_train, other_model.predict(X_train_tr))\n","other_test_mse = mean_squared_error(y_test, other_model.predict(X_test_tr))\n","print('Train Mean Squared Error:', other_train_mse)\n","print('Test Mean Squared Error:', other_test_mse)"]},{"cell_type":"markdown","metadata":{},"source":["This model initially seems great...0 MSE for the training data! But then you see that it is performing worse than our linear regression model on the test data. This model **is** overfitting."]},{"cell_type":"markdown","metadata":{},"source":["## Additional Resources\n","\n","[This blog post](https://towardsdatascience.com/linear-regression-in-python-9a1f5f000606) shows a walkthrough of the key steps for model validation with train-test split and scikit-learn."]},{"cell_type":"markdown","metadata":{},"source":["## Summary \n","\n","In this lesson, you learned the importance of the train-test split approach and used one of the most popular metrics for evaluating regression models, (R)MSE. You also saw how to use the `train_test_split` function from `sklearn` to split your data into training and test sets, and then evaluated whether models were overfitting using metrics on those training and test sets."]}],"metadata":{"kernelspec":{"display_name":"Python (learn-env)","language":"python","name":"learn-env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
